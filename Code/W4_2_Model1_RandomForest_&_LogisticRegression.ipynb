{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lexi-Zhou/stats201-project-zzz/blob/main/Code/W4_2_Model1_RandomForest_%26_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rzt3J8DrnQN2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw6BEU0OlNvs",
        "outputId": "aa0a39b5-170c-4709-f2a4-61efe763ef73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop Random Forest & Logistic Regression processing"
      ],
      "metadata": {
        "id": "2H2jorkF_f9z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3af034fd"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/Colab Notebooks/stats201_final_project/Week_4/'\n",
        "\n",
        "file_dict = {\n",
        "    'df1_stem_poor': '18_RMP_stem_poor.csv',\n",
        "    'df2_stem_average': '18_RMP_stem_average.csv',\n",
        "    'df3_stem_good': '18_RMP_stem_good.csv',\n",
        "    'df4_humanities_poor': '18_RMP_humanities_poor.csv',\n",
        "    'df5_humanities_average': '18_RMP_humanities_average.csv',\n",
        "    'df6_humanities_good': '18_RMP_humanities_good.csv'\n",
        "}\n",
        "\n",
        "all_results = {}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eafd97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c99ec81-99b7-4b9d-de5e-b1f75659310d"
      },
      "source": [
        "def process_and_store_results(df, comment_col):\n",
        "    # 1. Extract X and y\n",
        "    X = df[comment_col].fillna('')\n",
        "    y = df['prof_gender_finalized']\n",
        "\n",
        "    # 2. Initialize and fit TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer(max_features=2000)\n",
        "    X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "    # 3. Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_vectorized, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4. Train RandomForestClassifier\n",
        "    rf_model = RandomForestClassifier(random_state=42, oob_score=True)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Train LogisticRegression model\n",
        "    lr_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "    lr_model.fit(X_train, y_train)\n",
        "\n",
        "    # 6. Report model performance and capture metrics\n",
        "    y_pred_rf = rf_model.predict(X_test)\n",
        "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    oob_score_rf = rf_model.oob_score_\n",
        "    classification_report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "    y_pred_lr = lr_model.predict(X_test)\n",
        "    accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "    classification_report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "    # Print reports (for visibility during execution)\n",
        "    print(f\"--- RandomForestClassifier Performance for {comment_col} ---\")\n",
        "    print(f\"Random Forest Accuracy: {accuracy_rf:.2%}\")\n",
        "    print(f\"Random Forest OOB Score: {oob_score_rf:.2f}\")\n",
        "    print(\"Random Forest Classification Report:\")\n",
        "    print(classification_report_rf)\n",
        "\n",
        "    print(f\"\\n--- LogisticRegression Performance for {comment_col} ---\")\n",
        "    print(f\"Logistic Regression Accuracy: {accuracy_lr:.2%}\")\n",
        "    print(\"Logistic Regression Classification Report:\")\n",
        "    print(classification_report_lr)\n",
        "\n",
        "    # 7. Extract feature importances from Random Forest\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    rf_importance_df = pd.DataFrame({\n",
        "        'Word': feature_names,\n",
        "        'Feature Importance': feature_importances\n",
        "    }).sort_values(by='Feature Importance', ascending=False)\n",
        "\n",
        "    # 8. Extract coefficients from Logistic Regression\n",
        "    coefficients = lr_model.coef_[0]\n",
        "    positive_class = lr_model.classes_[1]\n",
        "    negative_class = lr_model.classes_[0]\n",
        "\n",
        "    directions = []\n",
        "    for coef in coefficients:\n",
        "        if coef > 0:\n",
        "            directions.append(f\"{positive_class.capitalize()}\")\n",
        "        elif coef < 0:\n",
        "            directions.append(f\"{negative_class.capitalize()}\")\n",
        "        else:\n",
        "            directions.append(\"Neutral\")\n",
        "\n",
        "    lr_direction_df = pd.DataFrame({\n",
        "        'Word': feature_names,\n",
        "        'Coefficient': coefficients,\n",
        "        'Direction': directions\n",
        "    }).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "    # 9. Merge and sort DataFrames\n",
        "    merged_df = pd.merge(rf_importance_df, lr_direction_df, on='Word', how='inner')\n",
        "    merged_df_sorted = merged_df.sort_values(by='Feature Importance', ascending=False)\n",
        "\n",
        "    return merged_df_sorted, classification_report_rf, classification_report_lr, accuracy_rf, oob_score_rf, accuracy_lr\n",
        "\n",
        "print(\"The 'process_and_store_results' function has been redefined to return additional metrics.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'process_and_store_results' function has been redefined to return additional metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bd6292c"
      },
      "source": [
        "def process_individual_dataframe(file_path, dataset_name):\n",
        "    import pandas as pd\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    print(f\"\\n--- Processing {dataset_name} (Original Comments) ---\")\n",
        "    # Process original comments\n",
        "    merged_original_df, rf_report_orig, lr_report_orig, rf_acc_orig, rf_oob_orig, lr_acc_orig = process_and_store_results(df, 'comments_original')\n",
        "    print(f\"\\nTop 20 Words for {dataset_name} (original comments):\")\n",
        "    print(merged_original_df.head(20))\n",
        "\n",
        "    print(f\"\\n--- Processing {dataset_name} (Scrubbed Comments) ---\")\n",
        "    # Process scrubbed comments\n",
        "    merged_scrubbed_df, rf_report_scrub, lr_report_scrub, rf_acc_scrub, rf_oob_scrub, lr_acc_scrub = process_and_store_results(df, 'comments_scrubbed')\n",
        "    print(f\"\\nTop 20 Words for {dataset_name} (scrubbed comments):\")\n",
        "    print(merged_scrubbed_df.head(20))\n",
        "\n",
        "    return (\n",
        "        merged_original_df,\n",
        "        merged_scrubbed_df,\n",
        "        {\n",
        "            'original': {\n",
        "                'rf_report': rf_report_orig,\n",
        "                'lr_report': lr_report_orig,\n",
        "                'rf_accuracy': rf_acc_orig,\n",
        "                'rf_oob_score': rf_oob_orig,\n",
        "                'lr_accuracy': lr_acc_orig,\n",
        "            },\n",
        "            'scrubbed': {\n",
        "                'rf_report': rf_report_scrub,\n",
        "                'lr_report': lr_report_scrub,\n",
        "                'rf_accuracy': rf_acc_scrub,\n",
        "                'rf_oob_score': rf_oob_scrub,\n",
        "                'lr_accuracy': lr_acc_scrub,\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"The 'process_individual_dataframe' function has been redefined to handle additional metrics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db4f87f"
      },
      "source": [
        "**Reasoning**:\n",
        "Since both `process_and_store_results` and `process_individual_dataframe` functions have been redefined to handle additional metrics, I need to re-run the loop that iterates through all datasets to populate the `all_results` dictionary with the new structured data containing these metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21e683e"
      },
      "source": [
        "for label, filename in file_dict.items():\n",
        "    full_file_path = os.path.join(base_path, filename)\n",
        "\n",
        "    print(f\"\\n\\n=======================================================\")\n",
        "    print(f\"Processing dataset: {label} (File: {filename})\")\n",
        "    print(f\"=======================================================\")\n",
        "\n",
        "    merged_original_df, merged_scrubbed_df, metrics_dict = process_individual_dataframe(full_file_path, label)\n",
        "\n",
        "    all_results[label] = {\n",
        "        'original': merged_original_df,\n",
        "        'scrubbed': merged_scrubbed_df,\n",
        "        'metrics': metrics_dict\n",
        "    }\n",
        "\n",
        "print(\"All datasets processed and results stored in 'all_results'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ebd9f23"
      },
      "source": [
        "# Output: txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295b7633"
      },
      "source": [
        "output_file_name = '/content/drive/MyDrive/Colab Notebooks/stats201_final_project/Week_4/19_combined_top_20_words_RFanalysis.txt'\n",
        "\n",
        "with open(output_file_name, 'w') as f:\n",
        "    for label, results_dict in all_results.items():\n",
        "        f.write(f\"========================================================\\n\")\n",
        "        f.write(f\"--- Analysis for {label} ---\\n\")\n",
        "        f.write(f\"========================================================\\n\\n\")\n",
        "\n",
        "        for comment_type in ['original', 'scrubbed']:\n",
        "            f.write(f\"--------------------------------------\\n\")\n",
        "            f.write(f\"--- {comment_type.capitalize()} Comments ---\\n\")\n",
        "            f.write(f\"--------------------------------------\\n\")\n",
        "\n",
        "            # Extract and write performance metrics\n",
        "            metrics = results_dict['metrics'][comment_type]\n",
        "            f.write(f\"Random Forest Accuracy: {metrics['rf_accuracy']:.2%}\\n\")\n",
        "            f.write(f\"Random Forest OOB Score: {metrics['rf_oob_score']:.2f}\\n\")\n",
        "            f.write(f\"Logistic Regression Accuracy: {metrics['lr_accuracy']:.2%}\\n\\n\")\n",
        "\n",
        "            f.write(\"Random Forest Classification Report:\\n\")\n",
        "            f.write(metrics['rf_report'])\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"Logistic Regression Classification Report:\\n\")\n",
        "            f.write(metrics['lr_report'])\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Extract and write top 20 words\n",
        "            df_to_write = results_dict[comment_type].head(20)\n",
        "\n",
        "            f.write(\"Top 20 Words:\\n\")\n",
        "            f.write(f\"{'Word':<20} {'Feature Importance':<20} {'Coefficient':<15} {'Direction':<10}\\n\")\n",
        "            f.write(f\"{'':-<20} {'':-<20} {'':-<15} {'':-<10}\\n\")\n",
        "            for index, row in df_to_write.iterrows():\n",
        "                f.write(f\"{row['Word']:<20} {row['Feature Importance']:<20.6f} {row['Coefficient']:<15.6f} {row['Direction']:<10}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Comprehensive report written to {output_file_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9cc4df7"
      },
      "source": [
        "# Output: Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182b075"
      },
      "source": [
        "top_scrubbed_words_coefficients = {}\n",
        "\n",
        "for label, results_dict in all_results.items():\n",
        "    # Access the 'scrubbed' DataFrame\n",
        "    scrubbed_df = results_dict['scrubbed']\n",
        "\n",
        "    # Select the top 20 rows\n",
        "    top_20_scrubbed = scrubbed_df.head(20)\n",
        "\n",
        "    # Extract 'Word' and 'Coefficient' columns\n",
        "    words_and_coefficients = top_20_scrubbed[['Word', 'Coefficient']]\n",
        "\n",
        "    # Store in the new dictionary\n",
        "    top_scrubbed_words_coefficients[label] = words_and_coefficients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "789826de"
      },
      "source": [
        "all_words_coeffs_dfs = []\n",
        "for label, df in top_scrubbed_words_coefficients.items():\n",
        "    # Rename 'Coefficient' column to the dataset label for merging\n",
        "    df_temp = df.rename(columns={'Coefficient': label}).set_index('Word')\n",
        "    all_words_coeffs_dfs.append(df_temp)\n",
        "\n",
        "# Concatenate all DataFrames. Use 'outer' join to include all unique words\n",
        "heatmap_data_coeffs = pd.concat(all_words_coeffs_dfs, axis=1, join='outer')\n",
        "\n",
        "# Fill NaN values with 0, as a word not being in the top 20 for a dataset implies a coefficient of 0 for the heatmap's color\n",
        "heatmap_data_coeffs = heatmap_data_coeffs.fillna(0)\n",
        "\n",
        "# Sort words alphabetically for consistent heatmap presentation\n",
        "heatmap_data_coeffs = heatmap_data_coeffs.sort_index()\n",
        "\n",
        "print(\"Prepared heatmap_data_coeffs DataFrame:\")\n",
        "print(heatmap_data_coeffs.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68b7735b"
      },
      "source": [
        "top_scrubbed_words_importances = {}\n",
        "\n",
        "for label, results_dict in all_results.items():\n",
        "    # Access the 'scrubbed' DataFrame\n",
        "    scrubbed_df = results_dict['scrubbed']\n",
        "\n",
        "    # Select the top 20 rows\n",
        "    top_20_scrubbed = scrubbed_df.head(20)\n",
        "\n",
        "    # Extract 'Word' and 'Feature Importance' columns\n",
        "    words_and_importances = top_20_scrubbed[['Word', 'Feature Importance']]\n",
        "\n",
        "    # Store in the new dictionary\n",
        "    top_scrubbed_words_importances[label] = words_and_importances\n",
        "\n",
        "all_words_importances_dfs = []\n",
        "for label, df in top_scrubbed_words_importances.items():\n",
        "    # Rename 'Feature Importance' column to the dataset label for merging\n",
        "    df_temp = df.rename(columns={'Feature Importance': label}).set_index('Word')\n",
        "    all_words_importances_dfs.append(df_temp)\n",
        "\n",
        "# Concatenate all DataFrames. Use 'outer' join to include all unique words\n",
        "heatmap_data_importances = pd.concat(all_words_importances_dfs, axis=1, join='outer')\n",
        "\n",
        "# Fill NaN values with 0, as a word not being in the top 20 for a dataset implies no significant importance there.\n",
        "heatmap_data_importances = heatmap_data_importances.fillna(0)\n",
        "\n",
        "# Sort words alphabetically for consistent heatmap presentation\n",
        "heatmap_data_importances = heatmap_data_importances.sort_index()\n",
        "\n",
        "print(\"Prepared heatmap_data_importances DataFrame:\")\n",
        "print(heatmap_data_importances.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b189d0d"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate combined importance for each word across all datasets\n",
        "# Sum the importances across the rows (axis=1) to get a total importance for each word\n",
        "combined_importances = heatmap_data_importances.sum(axis=1)\n",
        "\n",
        "# Get the top 30 words based on combined importance\n",
        "top_30_overall_words = combined_importances.nlargest(30).index\n",
        "\n",
        "# Filter heatmap_data_coeffs and heatmap_data_importances to include only these top 30 words\n",
        "filtered_heatmap_data_coeffs = heatmap_data_coeffs.loc[top_30_overall_words]\n",
        "filtered_heatmap_data_importances = heatmap_data_importances.loc[top_30_overall_words]\n",
        "\n",
        "plt.figure(figsize=(14, 12)) # Adjusted figure size for better readability\n",
        "sns.heatmap(\n",
        "    filtered_heatmap_data_coeffs,\n",
        "    cmap='RdBu_r',\n",
        "    center=0,\n",
        "    annot=filtered_heatmap_data_importances,\n",
        "    fmt='.3f',\n",
        "    linewidths=.5,\n",
        "    linecolor='lightgray'\n",
        ")\n",
        "plt.title('Top 30 Overall Scrubbed Words: Coefficients (Color) and Feature Importances (Annotation)\\n Red: Male-leaning, Blue: Female-leaning', fontsize=16)\n",
        "plt.xlabel('Dataset', fontsize=12)\n",
        "plt.ylabel('Words', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Heatmap displayed with overall top 30 scrubbed words, coefficients as colors and feature importances as annotations.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFXho8I5Wox4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dc4141c"
      },
      "source": [
        "## Report on Current Models\n",
        "\n",
        "### How the Model Works?\n",
        "\n",
        "- **1. Data Preparation and Feature Extraction (TF-IDF Vectorization)**:\n",
        "    - **Word Counting & Vectorization**: Each dataset's original and scrubbed comments are processed. A `TfidfVectorizer` is initialized to convert text comments into a matrix of TF-IDF features. This process effectively counts words, weighs their frequency by how often they appear in a document relative to the corpus, and selects a predefined number of `max_features` (e.g., 2000) to focus on the most discriminative words. This helps prevent very common words from dominating the analysis.\n",
        "\n",
        "- **2. Model Training (Random Forest & Logistic Regression)**:\n",
        "    - **Data Splitting**: The vectorized data is split into training and testing sets (e.g., 80% train, 20% test) to evaluate model performance on unseen data.\n",
        "    - **Random Forest Classifier**: A `RandomForestClassifier` is trained on the training data. This ensemble method builds multiple decision trees and merges their predictions to improve accuracy and control overfitting.\n",
        "    - **Logistic Regression**: A `LogisticRegression` model is also trained. This is a linear model used for binary classification, which models the probability of a binary outcome.\n",
        "\n",
        "- **3. Performance Evaluation**:\n",
        "    - **Accuracy Score**: The accuracy of both models is calculated on the test set, showing the proportion of correctly classified instances.\n",
        "    - **OOB Score (Random Forest)**: For Random Forest, the Out-Of-Bag (OOB) score is computed, providing an internal estimate of generalization error without the need for a separate validation set.\n",
        "    - **Classification Reports**: Detailed classification reports are generated for both models, including precision, recall, f1-score, and support for each class.\n",
        "\n",
        "- **4. Feature Importance and Coefficients Extraction**:\n",
        "    - **Random Forest Feature Importance**: The `feature_importances_` attribute from the trained Random Forest model is extracted. This provides a score for each word (feature) indicating its relative importance in predicting the gender of the professor. Higher values mean more importance.\n",
        "    - **Logistic Regression Coefficients**: The `coef_[0]` attribute from the trained Logistic Regression model is extracted. These coefficients represent the log-odds change in the dependent variable for a one-unit change in the independent variable (word presence/TF-IDF score). The sign of the coefficient indicates the direction of influence:\n",
        "        - **Positive Coefficient**: Word is associated with the positive class (e.g., 'Male' leaning).\n",
        "        - **Negative Coefficient**: Word is associated with the negative class (e.g., 'Female' leaning).\n",
        "\n",
        "- **5. Heatmap Visualization**:\n",
        "    - **Combined Importance for Top Words**: For heatmap visualization, the `Feature Importance` from Random Forest and `Coefficient` from Logistic Regression are extracted for the top words from each dataset. An overall importance score for each word is calculated by summing its Random Forest importances across all datasets.\n",
        "    - **Top 30 Overall Words Selection**: The top 30 words based on this combined overall importance are selected.\n",
        "    - **Heatmap Generation**: A heatmap is generated where:\n",
        "        - **Color**: Represents the Logistic Regression `Coefficient` for the selected words across different datasets. A diverging colormap (e.g., 'RdBu_r' centered at 0) clearly shows positive (Red: Male-leaning) and negative (Blue: Female-leaning) associations.\n",
        "        - **Annotation**: The Random Forest `Feature Importance` values are annotated within each cell, providing a numerical measure of how impactful that word was in the Random Forest model for that specific dataset.\n",
        "\n",
        "### Key Findings (from the heatmap visualization):\n",
        "\n",
        "The heatmap displays the top 30 overall scrubbed words. The color of each cell indicates the direction of the word's association (red for male-leaning, blue for female-leaning) and the intensity of the color indicates the strength of this association, according to the Logistic Regression model. The numerical annotations within the cells show the Random Forest feature importance, indicating how much that word contributed to the predictive power of the Random Forest model. This allows for a comprehensive view of how different words are associated with professor gender across various academic disciplines and performance categories."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeZcNGLsbBNaAqBJtwrlac",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}